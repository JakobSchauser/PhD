{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up webscrabing\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# set up the url\n",
    "url = 'https://phdcourses.dk/Course/'\n",
    "\n",
    "\n",
    "# set up the headers\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "def get_page(url,):\n",
    "    page = requests.get(url, headers=headers)\n",
    "    if page.status_code == 200:\n",
    "        try:\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            return soup\n",
    "        except:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def get_data_from_id(num : int):\n",
    "    url = f'https://phdcourses.dk/Course/{num}'\n",
    "    soup = get_page(url)\n",
    "\n",
    "    if soup is None:\n",
    "        return None\n",
    "\n",
    "    def get_text_after_header(header_text):\n",
    "        header = soup.find(\"h3\", string=lambda text: text and header_text.lower() in text.lower())\n",
    "        return header.find_next(\"p\").text.strip() if header and header.find_next(\"p\") else \"Not found\"\n",
    "\n",
    "    def get_link_after_header(header_text):\n",
    "        header = soup.find(\"h3\", string=lambda text: text and header_text.lower() in text.lower())\n",
    "        link_tag = header.find_next(\"p\").find(\"a\") if header and header.find_next(\"p\") else None\n",
    "        return link_tag[\"href\"] if link_tag else \"No link found\"\n",
    "\n",
    "    def get_text_from_tag(tag, class_name):\n",
    "        element = soup.find(tag, class_=class_name)\n",
    "        return element.text.strip() if element else \"Not found\"\n",
    "\n",
    "    # Extract data with error handling\n",
    "    course_dates = get_text_after_header(\"Course dates\")\n",
    "    ects = get_text_after_header(\"ECTS\")\n",
    "    course_link = get_link_after_header(\"Link\")\n",
    "    title = get_text_from_tag(\"h2\", \"page__title\")\n",
    "    phd_school = get_text_from_tag(\"h3\", \"page__subtitle\")\n",
    "\n",
    "\n",
    "    def make_legal(text):\n",
    "        return text.replace('\"', '').replace(\"'\", \"\").replace(\";\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\").replace(\"\\r\", \"\").replace(\",\", \".\")\n",
    "    \n",
    "    title = make_legal(title)\n",
    "    phd_school = make_legal(phd_school)\n",
    "    course_dates = make_legal(course_dates)\n",
    "    ects = make_legal(ects).replace(\"points\", \"\")\n",
    "    course_link = make_legal(course_link)\n",
    "\n",
    "    return title, phd_school, course_dates, ects, course_link\n",
    "\n",
    "\n",
    "def get_ids_from_searchpagenum(pagenum : int):\n",
    "\n",
    "    soup = get_page(f\"https://phdcourses.dk/?page={pagenum}&currentSearchWord=&currentEcts=&\")\n",
    "\n",
    "    nums = []\n",
    "    t = soup.find_all(\"a\", class_ = \"subtitle\")\n",
    "    for i in t:\n",
    "        nums.append(i[\"href\"].split(\"/\")[-1])\n",
    "    return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This script will scrape data from phdcourses.dk\n",
      "Estimated time: 4 minutes\n",
      "Finding course IDs...\n",
      "\n",
      "\n",
      "Page 90\n",
      "\n",
      "Getting course data...\n",
      "\n",
      "\n",
      "Course 127737  - 99\n",
      "Done!\n",
      "Data saved to phd_courses.csv\n"
     ]
    }
   ],
   "source": [
    "sleeptime = 0.25\n",
    "N_pages = 90\n",
    "\n",
    "print(\"This script will scrape data from phdcourses.dk\")\n",
    "print(f\"Estimated time: {int((sleeptime*N_pages + sleeptime*N_pages*10)/60)} minutes\")\n",
    "\n",
    "print(\"Finding course IDs...\\n\\n\")\n",
    "\n",
    "ids = []\n",
    "for pn in range(1, N_pages+1):\n",
    "    print(f\"Page {pn}\", end = \"\\r\")\n",
    "    for num in get_ids_from_searchpagenum(pn):\n",
    "        ids.append(num)\n",
    "    time.sleep(sleeptime)\n",
    "\n",
    "print(\"\\n\\nGetting course data...\\n\")\n",
    "print(\"\")\n",
    "data = []\n",
    "\n",
    "for i, num in enumerate(ids):\n",
    "    print(f\"Course {num}  - {int(i/len(ids)*100.):d}/100+\n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \n",
    "          \", end = \"\\r\")\n",
    "    data_from_id = get_data_from_id(num)\n",
    "    if data_from_id is not None: \n",
    "        data.append(data_from_id)\n",
    "    else:\n",
    "        print(f\"Error with course {num}\")\n",
    "    time.sleep(sleeptime)\n",
    "print(\"\")\n",
    "print(\"Done!\")\n",
    "\n",
    "\n",
    "data = np.array(data)\n",
    "\n",
    "np.savetxt(\"phd_courses.csv\", data, delimiter=\",\", fmt=\"%s\")\n",
    "print(\"Data saved to phd_courses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to phd_courses.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = np.loadtxt(\"phd_courses.csv\", delimiter=\",\", dtype=str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
